{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-17T18:09:34.845922Z",
     "start_time": "2025-01-17T18:09:33.565949Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "\n",
    "from aidream_registration import constants\n",
    "from aidream_registration.dataloaders import AtlasImagingNiftiLoader\n",
    "from aidream_registration.utils.cohort_utils import get_perfusion_patients\n",
    "from sklearn.utils import resample\n",
    "\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset, Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import ants\n"
   ],
   "id": "549063a71e52ee69",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-17T18:09:34.850336Z",
     "start_time": "2025-01-17T18:09:34.848593Z"
    }
   },
   "cell_type": "code",
   "source": "torch.cuda.empty_cache()",
   "id": "fe8f70e697ee66b3",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-17T18:09:34.899770Z",
     "start_time": "2025-01-17T18:09:34.898206Z"
    }
   },
   "cell_type": "code",
   "source": [
    "list_patients = get_perfusion_patients()\n",
    "print(\"Number of patients:\", len(list_patients))\n"
   ],
   "id": "87b405eef8b2932",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of patients: 186\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-17T18:09:35.186690Z",
     "start_time": "2025-01-17T18:09:35.185058Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Load the atlas imaging nifti loader :\n",
    "atlas_loader = AtlasImagingNiftiLoader(source_mri=\"PIPELINE_SS\")\n"
   ],
   "id": "54d7e445203caad7",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-17T18:09:35.445648Z",
     "start_time": "2025-01-17T18:09:35.312981Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# First, load the hammersmith atlas, and create a mask :\n",
    "path_hammersmith = constants.DIR_DATA / \"hammersmith\" / \"T1w_ICBM_skullstripped.nii.gz\"\n",
    "ants_hammersmith = ants.image_read(str(path_hammersmith))\n",
    "\n",
    "mask_hammersmith = ants_hammersmith > 0\n",
    "print(f\"Hammersmith mask volume: {mask_hammersmith.sum() / 1e3:.2f} cm3\")\n"
   ],
   "id": "9910f78f82892064",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hammersmith mask volume: 1886.57 cm3\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-17T18:09:37.054371Z",
     "start_time": "2025-01-17T18:09:36.298174Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Load all segmentations in a dictionary :\n",
    "dict_segmentations = {\n",
    "    path_seg.name.removesuffix(\".nii.gz\"): ants.image_read(str(path_seg)) > 0\n",
    "    for path_seg in (constants.DIR_DATA / \"hammersmith\").glob(\"*.nii.gz\")\n",
    "    if path_seg.name != \"T1w_ICBM_skullstripped.nii.gz\"\n",
    "}\n",
    "\n",
    "print(\"Number of segmentations:\", len(dict_segmentations))\n",
    "\n",
    "for segmentation, mask_segmentation in dict_segmentations.items():\n",
    "    print(fr\"{segmentation} volume: {mask_segmentation.sum() / 1e3:.2f} cm3\")\n"
   ],
   "id": "1ac898ad7dab4e50",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of segmentations: 21\n",
      "L.Ventricles volume: 6.72 cm3\n",
      "R_insula volume: 16.48 cm3\n",
      "R.Ventricles volume: 6.11 cm3\n",
      "L.Temporal_lobe volume: 115.72 cm3\n",
      "R.Occipital_lobe volume: 76.46 cm3\n",
      "L.Grey_nuclei volume: 18.22 cm3\n",
      "R.Grey_nuclei volume: 18.20 cm3\n",
      "R.Limbic_lobe volume: 24.28 cm3\n",
      "Brainstem volume: 26.94 cm3\n",
      "L.frontal_lobe volume: 225.46 cm3\n",
      "third_ventricle volume: 0.63 cm3\n",
      "L.Post_fossea volume: 90.63 cm3\n",
      "L.Parietal_lobe volume: 142.35 cm3\n",
      "Corpus_callosum volume: 21.05 cm3\n",
      "R.Temporal_lobe volume: 121.76 cm3\n",
      "L.Occipital_lobe volume: 75.31 cm3\n",
      "R.frontal_lobe volume: 227.50 cm3\n",
      "R.Post_fossea volume: 88.57 cm3\n",
      "R.Parietal_lobe volume: 141.59 cm3\n",
      "L.Limbic_lobe volume: 24.96 cm3\n",
      "L_insula volume: 16.62 cm3\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-17T18:10:16.997826Z",
     "start_time": "2025-01-17T18:10:16.978819Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Create a custom ventricles mask by combining the lateral ventricles and the third ventricle :\n",
    "mask_ventricles = dict_segmentations[\"L.Ventricles\"] + dict_segmentations[\"R.Ventricles\"] + dict_segmentations[\"third_ventricle\"]\n",
    "mask_ventricles = mask_ventricles > 0\n",
    "\n",
    "print(f\"Ventricles volume: {mask_ventricles.sum() / 1e3:.2f} cm3\")\n",
    "\n",
    "path_ventricles = constants.DIR_DATA / \"hammersmith\" / \"custom\" / \"total_ventricles\" / \"ventricles.nii.gz\"\n",
    "path_ventricles.parent.mkdir(parents=True, exist_ok=True)\n",
    "mask_ventricles.to_file(str(path_ventricles))\n"
   ],
   "id": "95aa07790df841af",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ventricles volume: 13.47 cm3\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-17T18:10:19.458152Z",
     "start_time": "2025-01-17T18:10:19.208975Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Create a mask for the neighbors of the ventricles :\n",
    "dict_ventricles_neighbors = {}\n",
    "\n",
    "for seg in [\"L.Ventricles\", \"R.Ventricles\", \"third_ventricle\"]:\n",
    "\n",
    "    # Load the segmentation mask (prior) :\n",
    "    mask_segmentation = dict_segmentations[seg]\n",
    "    seg_idx = np.array(np.nonzero(mask_segmentation.numpy()))\n",
    "    # Bounding box :\n",
    "    bbox_min, bbox_max = np.min(seg_idx, axis=1), np.max(seg_idx, axis=1)\n",
    "    # Dilate by 3 voxels :\n",
    "    bbox_min, bbox_max = bbox_min - 3, bbox_max + 3\n",
    "    np_bbox = np.zeros(mask_segmentation.shape)\n",
    "    np_bbox[bbox_min[0]: bbox_max[0], bbox_min[1]: bbox_max[1], bbox_min[2]: bbox_max[2]] = 1\n",
    "\n",
    "    dict_ventricles_neighbors[seg] = (ants.from_numpy(np_bbox, origin=mask_segmentation.origin, spacing=mask_segmentation.spacing, direction=mask_segmentation.direction) > 0) * mask_hammersmith\n",
    "\n",
    "# Create mask_ventricles_neighborhood :\n",
    "mask_ventricles_neighbors = dict_ventricles_neighbors[\"L.Ventricles\"] + dict_ventricles_neighbors[\"R.Ventricles\"] + dict_ventricles_neighbors[\"third_ventricle\"]\n",
    "mask_ventricles_neighbors = mask_ventricles_neighbors > 0\n",
    "\n",
    "print(f\"Ventricles neighborhood volume: {mask_ventricles_neighbors.sum() / 1e3:.2f} cm3\")\n",
    "\n",
    "# Create mask for brain without the ventricles' neighborhood :\n",
    "mask_no_ventricles_neighbors = ants_hammersmith * (1 - mask_ventricles_neighbors)\n",
    "mask_no_ventricles_neighbors = mask_no_ventricles_neighbors > 0\n",
    "\n",
    "print(f\"Brain without ventricles neighborhood volume: {mask_no_ventricles_neighbors.sum() / 1e3:.2f} cm3\")\n"
   ],
   "id": "d1421c7cc8bdcc20",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ventricles neighborhood volume: 440.61 cm3\n",
      "Brain without ventricles neighborhood volume: 1445.96 cm3\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-17T18:10:20.619436Z",
     "start_time": "2025-01-17T18:10:20.459294Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Create a CSF segmentation for the voxels outside the ventricles that are not segmented :\n",
    "mask_csf_no_ventricles_neighbors = mask_no_ventricles_neighbors\n",
    "\n",
    "for seg, mask_segmentation in dict_segmentations.items():\n",
    "\n",
    "    if seg not in [\"L.Ventricles\", \"R.Ventricles\", \"third_ventricle\"]:\n",
    "        mask_csf_no_ventricles_neighbors = mask_csf_no_ventricles_neighbors * (1 - mask_segmentation)\n",
    "\n",
    "assert set(np.unique(mask_csf_no_ventricles_neighbors.numpy())) == {0, 1}\n",
    "\n",
    "print(f\"CSF volume outside of the ventricles neighborhood: {mask_csf_no_ventricles_neighbors.sum() / 1e3:.2f} cm3\")\n",
    "\n",
    "# Create a mask for the segmentations excluding the ventricles and the CSF :\n",
    "mask_no_csf_no_ventricles_neighbors = mask_no_ventricles_neighbors * (1 - mask_csf_no_ventricles_neighbors)\n",
    "\n",
    "print(f\"Brain without ventricles and CSF volume: {mask_no_csf_no_ventricles_neighbors.sum() / 1e3:.2f} cm3\")\n"
   ],
   "id": "2cb8952ba7e4687",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSF volume outside of the ventricles neighborhood: 259.03 cm3\n",
      "Brain without ventricles and CSF volume: 1186.93 cm3\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-17T18:10:21.393928Z",
     "start_time": "2025-01-17T18:10:21.313866Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Generate spatial coordinate images :\n",
    "x,y,z = ants_hammersmith.shape\n",
    "\n",
    "x_coords = np.linspace(0, 1, x)[:, None, None]\n",
    "y_coords = np.linspace(0, 1, y)[None, :, None]\n",
    "z_coords = np.linspace(0, 1, z)[None, None, :]\n",
    "\n",
    "x_map = np.broadcast_to(x_coords, (x, y, z))\n",
    "y_map = np.broadcast_to(y_coords, (x, y, z))\n",
    "z_map = np.broadcast_to(z_coords, (x, y, z))\n",
    "\n",
    "ants_x_map = ants.from_numpy(x_map, origin=ants_hammersmith.origin, spacing=ants_hammersmith.spacing, direction=ants_hammersmith.direction)\n",
    "ants_y_map = ants.from_numpy(y_map, origin=ants_hammersmith.origin, spacing=ants_hammersmith.spacing, direction=ants_hammersmith.direction)\n",
    "ants_z_map = ants.from_numpy(z_map, origin=ants_hammersmith.origin, spacing=ants_hammersmith.spacing, direction=ants_hammersmith.direction)\n",
    "\n"
   ],
   "id": "9f2bbd27afe56c25",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-17T18:18:22.566311Z",
     "start_time": "2025-01-17T18:16:00.476926Z"
    }
   },
   "cell_type": "code",
   "source": [
    "radius = 2\n",
    "beta = 0.3\n",
    "\n",
    "mrf = f\"[{beta}, {radius}x{radius}x{radius}]\"\n",
    "\n",
    "print(fr\"Applying Atropos with MRF = {mrf} ...\")\n",
    "\n",
    "atropos_results = ants.atropos(a=[ants_hammersmith],\n",
    "                               x=mask_hammersmith,\n",
    "                               m=mrf,\n",
    "                               c=\"[10,0]\",\n",
    "                               i=[mask_ventricles, mask_csf_no_ventricles_neighbors, mask_no_csf_no_ventricles_neighbors],\n",
    "                               verbose=1)\n",
    "\n",
    "# ants_ventricles_atropos = (atropos_results[\"segmentation\"] == 1) * mask_ventricles_neighbors\n",
    "ants_ventricles_atropos = atropos_results[\"segmentation\"]\n",
    "\n",
    "path_ventricles_atropos = constants.DIR_DATA / \"hammersmith\" / \"custom\" / f\"ventricle_atropos_{mrf}.nii.gz\"\n",
    "path_ventricles_atropos.parent.mkdir(parents=True, exist_ok=True)\n",
    "ants_ventricles_atropos.to_file(str(path_ventricles_atropos))\n"
   ],
   "id": "7088a5998ceca12e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying Atropos with MRF = [0.3, 2x2x2] ...\n",
      "\n",
      "Running Atropos for 3-dimensional images.\n",
      "\n",
      "Progress: \n",
      "  Iteration 0 (of 10): posterior probability = 0 (annealing temperature = 1)\n",
      "  Iteration 1 (of 10): posterior probability = 0.980294 (annealing temperature = 1)\n",
      "  Iteration 2 (of 10): posterior probability = 0.992723 (annealing temperature = 1)\n",
      "  Iteration 3 (of 10): posterior probability = 0.993611 (annealing temperature = 1)\n",
      "  Iteration 4 (of 10): posterior probability = 0.99427 (annealing temperature = 1)\n",
      "  Iteration 5 (of 10): posterior probability = 0.994788 (annealing temperature = 1)\n",
      "  Iteration 6 (of 10): posterior probability = 0.99518 (annealing temperature = 1)\n",
      "  Iteration 7 (of 10): posterior probability = 0.995501 (annealing temperature = 1)\n",
      "  Iteration 8 (of 10): posterior probability = 0.995698 (annealing temperature = 1)\n",
      "  Iteration 9 (of 10): posterior probability = 0.995889 (annealing temperature = 1)\n",
      "\n",
      "Writing output:\n",
      "  Writing posterior image (class 1)\n",
      "  Writing posterior image (class 2)\n",
      "  Writing posterior image (class 3)\n",
      "\n",
      "  AtroposSegmentationImageFilter (0x418815a0)\n",
      "    RTTI typeinfo:   itk::ants::AtroposSegmentationImageFilter<itk::Image<float, 3u>, itk::Image<unsigned int, 3u>, itk::Image<unsigned int, 3u> >\n",
      "    Reference Count: 1\n",
      "    Modified Time: 5085899\n",
      "    Debug: Off\n",
      "    Object Name: \n",
      "    Observers: \n",
      "      IterationEvent(Command)\n",
      "    Inputs: \n",
      "      Primary: (0x41852e80) *\n",
      "      _1: (0x417ce900)\n",
      "      _2: (0x41740860)\n",
      "    Indexed Inputs: \n",
      "      0: Primary (0x41852e80)\n",
      "      1: _1 (0x417ce900)\n",
      "      2: _2 (0x41740860)\n",
      "    Required Input Names: Primary\n",
      "    NumberOfRequiredInputs: 1\n",
      "    Outputs: \n",
      "      Primary: (0x4ffe7990)\n",
      "    Indexed Outputs: \n",
      "      0: Primary (0x4ffe7990)\n",
      "    NumberOfRequiredOutputs: 1\n",
      "    Number Of Work Units: 128\n",
      "    ReleaseDataFlag: Off\n",
      "    ReleaseDataBeforeUpdateFlag: Off\n",
      "    AbortGenerateData: Off\n",
      "    Progress: 0\n",
      "    Multithreader: \n",
      "      RTTI typeinfo:   itk::PoolMultiThreader\n",
      "      Reference Count: 1\n",
      "      Modified Time: 4149936\n",
      "      Debug: Off\n",
      "      Object Name: \n",
      "      Observers: \n",
      "        none\n",
      "      Number of Work Units: 128\n",
      "      Number of Threads: 32\n",
      "      Global Maximum Number Of Threads: 128\n",
      "      Global Default Number Of Threads: 32\n",
      "      Global Default Threader Type: itk::MultiThreaderBaseEnums::Threader::Pool\n",
      "      SingleMethod: 0\n",
      "      SingleData: 0\n",
      "    DynamicMultiThreading: On\n",
      "    CoordinateTolerance: 1e-06\n",
      "    DirectionTolerance: 1e-06\n",
      "    Maximum number of iterations: 10\n",
      "    Convergence threshold: 0\n",
      "    Number of tissue classes: 3\n",
      "    Number of partial volume classes: 0\n",
      "    Minimize memory usage: false\n",
      "    Initialization strategy: Prior probability images\n",
      "      Use Euclidean distance for prior labels: false\n",
      "    Posterior probability formulation: Socrates\n",
      "      initial annealing temperature = 1\n",
      "      annealing rate = 1\n",
      "      minimum annealing temperature = 0.1\n",
      "    MRF parameters\n",
      "      MRF smoothing factor = 0.3\n",
      "      MRF radius = [2, 2, 2]\n",
      "    Use asynchronous updating of the labels.\n",
      "      ICM parameters\n",
      "        maximum ICM code = 44\n",
      "        maximum number of ICM iterations = 1\n",
      "    No outlier handling.\n",
      "    Adaptive smoothing weights: [0]\n",
      "    B-spline smoothing\n",
      "      spline order = 3\n",
      "      number of levels = [6, 6, 6]\n",
      "      number of initial control points = [4, 4, 4]\n",
      "    Tissue class 1: proportion = 0.0178427\n",
      "      GaussianListSampleFunction (0x422091d0)\n",
      "        mean = [33.4700999318487], covariance = [57.175]\n",
      "    Tissue class 2: proportion = 0.101096\n",
      "      GaussianListSampleFunction (0x416a9f80)\n",
      "        mean = [47.29331749724246], covariance = [154.871]\n",
      "    Tissue class 3: proportion = 0.881044\n",
      "      GaussianListSampleFunction (0x417d3020)\n",
      "        mean = [69.94864822737448], covariance = [119.342]\n",
      "Elapsed time: 140.144\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-17T18:20:51.890927Z",
     "start_time": "2025-01-17T18:18:22.583475Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Apply atropos to improve the quality fo the ventricles segmentation using the ventricles mask as a prior :\n",
    "\n",
    "radius = 2\n",
    "beta = 0.3\n",
    "\n",
    "mrf = f\"[{beta}, {radius}x{radius}x{radius}]\"\n",
    "\n",
    "print(fr\"Applying Atropos with MRF = {mrf} ...\")\n",
    "\n",
    "atropos_results = ants.atropos(a=[ants_hammersmith, ants_x_map, ants_y_map],\n",
    "                               x=mask_hammersmith,\n",
    "                               m=mrf,\n",
    "                               c=\"[10,0]\",\n",
    "                               i=[mask_ventricles, mask_csf_no_ventricles_neighbors, mask_no_csf_no_ventricles_neighbors],\n",
    "                               verbose=1)\n",
    "\n",
    "# ants_ventricles_atropos = (atropos_results[\"segmentation\"] == 1) * mask_ventricles_neighbors\n",
    "ants_ventricles_atropos = atropos_results[\"segmentation\"]\n",
    "\n",
    "path_ventricles_atropos = constants.DIR_DATA / \"hammersmith\" / \"custom\" / f\"ventricle_atropos_{mrf}_coords.nii.gz\"\n",
    "path_ventricles_atropos.parent.mkdir(parents=True, exist_ok=True)\n",
    "ants_ventricles_atropos.to_file(str(path_ventricles_atropos))"
   ],
   "id": "a1484023dd97624d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying Atropos with MRF = [0.3, 2x2x2] ...\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-17T17:21:00.099189Z",
     "start_time": "2025-01-17T17:20:59.951111Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Now, let's treat the problem as a Machine Learning classification problem :\n",
    "# The training data will be the ventricles' segmentation and the voxels outside the ventricles' neighborhood.\n",
    "# The prediction will be done on the voxels inside the ventricles' neighborhood, that are not segmented as ventricles.\n",
    "\n",
    "# First, create the training data :\n",
    "# Add the ventricles' segmentation :\n",
    "ventricles_idx = np.array(np.nonzero(mask_ventricles.numpy()))\n",
    "ventricles_intensity = ants_hammersmith.numpy()[ventricles_idx[0], ventricles_idx[1], ventricles_idx[2]]\n",
    "df_ventricles = pd.DataFrame({\"x\": ventricles_idx[0], \"y\": ventricles_idx[1], \"z\": ventricles_idx[2],\n",
    "                              \"intensity\": ventricles_intensity, \"label\": 0})\n",
    "\n",
    "# now Add the voxels outside the ventricles' neighborhood that are CSF :\n",
    "csf_no_ventricles_neighbors_idx = np.array(np.nonzero(mask_csf_no_ventricles_neighbors.numpy()))\n",
    "csf_no_ventricles_neighbors_intensity = ants_hammersmith.numpy()[csf_no_ventricles_neighbors_idx[0], csf_no_ventricles_neighbors_idx[1], csf_no_ventricles_neighbors_idx[2]]\n",
    "df_csf_no_ventricles_neighbors = pd.DataFrame({\"x\": csf_no_ventricles_neighbors_idx[0], \"y\": csf_no_ventricles_neighbors_idx[1], \"z\": csf_no_ventricles_neighbors_idx[2],\n",
    "                                               \"intensity\": csf_no_ventricles_neighbors_intensity, \"label\": 1})\n",
    "\n",
    "# Now, add the voxels outside the ventricles' neighborhood that are not CSF :\n",
    "no_csf_no_ventricles_neighbors_idx = np.array(np.nonzero(mask_no_csf_no_ventricles_neighbors.numpy()))\n",
    "no_csf_no_ventricles_neighbors_intensity = ants_hammersmith.numpy()[no_csf_no_ventricles_neighbors_idx[0], no_csf_no_ventricles_neighbors_idx[1], no_csf_no_ventricles_neighbors_idx[2]]\n",
    "df_no_csf_no_ventricles_neighbors = pd.DataFrame({\"x\": no_csf_no_ventricles_neighbors_idx[0], \"y\": no_csf_no_ventricles_neighbors_idx[1], \"z\": no_csf_no_ventricles_neighbors_idx[2],\n",
    "                                                  \"intensity\": no_csf_no_ventricles_neighbors_intensity, \"label\": 2})\n",
    "\n",
    "# Concatenate the dataframes :\n",
    "df_train = pd.concat([df_ventricles, df_csf_no_ventricles_neighbors, df_no_csf_no_ventricles_neighbors], ignore_index=True)\n",
    "print(fr\"len(df_train): {len(df_train)}\")\n",
    "\n",
    "X_train, y_train = df_train[[\"x\", \"y\", \"z\", \"intensity\"]].values, df_train[\"label\"].values\n",
    "\n",
    "print(f\"Number of training samples: {len(X_train)}\")\n",
    "\n",
    "print(f\"percentage of ventricles in the training set: {np.mean(y_train == 0):.2f}\")\n",
    "print(f\"percentage of CSF in the training set: {np.mean(y_train == 1):.2f}\")\n",
    "print(f\"percentage of brain in the training set: {np.mean(y_train == 2):.2f}\")\n"
   ],
   "id": "aafee02baf39ce67",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(df_train): 1459429\n",
      "Number of training samples: 1459429\n",
      "percentage of ventricles in the training set: 0.01\n",
      "percentage of CSF in the training set: 0.18\n",
      "percentage of brain in the training set: 0.81\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-17T17:24:09.932057Z",
     "start_time": "2025-01-17T17:24:09.751560Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Let's resample the training data to balance the classes :\n",
    "ventricles = df_train[df_train[\"label\"] == 0]\n",
    "csf = df_train[df_train[\"label\"] == 1]\n",
    "brain = df_train[df_train[\"label\"] == 2]\n",
    "\n",
    "# the desired distribution is 2:2:6\n",
    "desired_ventricles_size = desired_csf_size = int(len(df_train) * 0.2)\n",
    "desired_brain_size = len(df_train) - desired_ventricles_size - desired_csf_size\n",
    "\n",
    "# Oversampling the ventricles and the CSF :\n",
    "ventricles_resampled = resample(ventricles, n_samples=desired_ventricles_size, replace=True, random_state=42)\n",
    "csf_resampled = resample(csf, n_samples=desired_csf_size, replace=True, random_state=42)\n",
    "\n",
    "# Downsampling the brain :\n",
    "brain_resampled = resample(brain, n_samples=desired_brain_size, replace=False, random_state=42)\n",
    "\n",
    "# Concatenate the resampled dataframes :\n",
    "df_train_resampled = pd.concat([ventricles_resampled, csf_resampled, brain_resampled], ignore_index=True)\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(df_train_resampled[[\"x\", \"y\", \"z\", \"intensity\"]].values, df_train_resampled[\"label\"].values, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"Number of training samples: {len(X_train)}\")\n",
    "print(f\"Number of validation samples: {len(X_val)}\")\n",
    "\n",
    "print(f\"percentage of ventricles in the training set: {np.mean(y_train == 0):.2f}, and in the validation set: {np.mean(y_val == 0):.2f}\")\n",
    "print(f\"percentage of CSF in the training set: {np.mean(y_train == 1):.2f}, and in the validation set: {np.mean(y_val == 1):.2f}\")\n",
    "print(f\"percentage of brain in the training set: {np.mean(y_train == 2):.2f}, and in the validation set: {np.mean(y_val == 2):.2f}\")\n"
   ],
   "id": "38a9197c1957df46",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training samples: 1167543\n",
      "Number of validation samples: 291886\n",
      "percentage of ventricles in the training set: 0.20, and in the validation set: 0.20\n",
      "percentage of CSF in the training set: 0.20, and in the validation set: 0.20\n",
      "percentage of brain in the training set: 0.60, and in the validation set: 0.60\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-17T17:24:45.945199Z",
     "start_time": "2025-01-17T17:24:45.899913Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Let's do the prediction set on the voxels inside the ventricles' neighborhood  that are not segmented as ventricles :\n",
    "mask_pred = mask_ventricles_neighbors * (1 - mask_ventricles)\n",
    "\n",
    "pred_idx = np.array(np.nonzero(mask_pred.numpy()))\n",
    "pred_intensity = ants_hammersmith.numpy()[pred_idx[0], pred_idx[1], pred_idx[2]]\n",
    "df_pred = pd.DataFrame({\"x\": pred_idx[0], \"y\": pred_idx[1], \"z\": pred_idx[2], \"intensity\": pred_intensity})\n",
    "\n",
    "X_pred = df_pred[[\"x\", \"y\", \"z\", \"intensity\"]].values\n",
    "print(f\"Nuber of prediction samples: {len(X_pred)} / {mask_ventricles_neighbors.sum()}\")\n"
   ],
   "id": "42113cfc438dcbd7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nuber of prediction samples: 427148 / 440611\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-17T17:24:48.702974Z",
     "start_time": "2025-01-17T17:24:48.700204Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Build the neural network classifier :\n",
    "class VoxelClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VoxelClassifier, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(4, 8),  # Input: 4 features, first hidden layer: 8 neurons\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(8, 16),  # Second hidden layer: 16 neurons\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16, 32), # Third hidden layer: 33 neurons\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 64), # Fourth hidden layer: 64 neurons\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32),  # Fifth hidden layer: 32 neurons\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 16),  # Sixth hidden layer: 16 neurons\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16, 3)    # Output: 3 classes\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n"
   ],
   "id": "2d83f44c3dab7381",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-17T17:24:50.130098Z",
     "start_time": "2025-01-17T17:24:49.908663Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model = VoxelClassifier()\n",
    "\n",
    "# Check if GPU is available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "\n",
    "# Move model to GPU\n",
    "model.to(device)\n"
   ],
   "id": "ad609f42720d09bf",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "VoxelClassifier(\n",
       "  (model): Sequential(\n",
       "    (0): Linear(in_features=4, out_features=8, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=8, out_features=16, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): Linear(in_features=16, out_features=32, bias=True)\n",
       "    (5): ReLU()\n",
       "    (6): Linear(in_features=32, out_features=64, bias=True)\n",
       "    (7): ReLU()\n",
       "    (8): Linear(in_features=64, out_features=32, bias=True)\n",
       "    (9): ReLU()\n",
       "    (10): Linear(in_features=32, out_features=16, bias=True)\n",
       "    (11): ReLU()\n",
       "    (12): Linear(in_features=16, out_features=3, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-17T17:25:17.012345Z",
     "start_time": "2025-01-17T17:25:16.992225Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Convert your training data into PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32).to(device)  # Features\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.long).to(device)    # Labels\n",
    "\n",
    "# Convert your validation data into PyTorch tensors\n",
    "X_val_tensor = torch.tensor(X_val, dtype=torch.float32).to(device)  # Features\n",
    "y_val_tensor = torch.tensor(y_val, dtype=torch.long).to(device)    # Labels\n"
   ],
   "id": "e2e14b2a7531342e",
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-17T17:28:24.567101Z",
     "start_time": "2025-01-17T17:28:24.022429Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "\n",
    "batch_size = 2000\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=100000, shuffle=False)\n",
    "\n",
    "# Verify DataLoaders by iterating through one batch\n",
    "for X_batch, y_batch in train_loader:\n",
    "    print(\"Training Batch:\")\n",
    "    print(\"  Features Shape:\", X_batch.shape)\n",
    "    print(\"  Labels Shape:\", y_batch.shape)\n",
    "    print(\"  Label Counts:\", torch.bincount(y_batch))\n",
    "    break  # Check only the first batch\n",
    "\n",
    "for X_batch, y_batch in val_loader:\n",
    "    print(\"Validation Batch:\")\n",
    "    print(\"  Features Shape:\", X_batch.shape)\n",
    "    print(\"  Labels Shape:\", y_batch.shape)\n",
    "    print(\"  Label Counts:\", torch.bincount(y_batch))\n",
    "    break  # Check only the first batch\n"
   ],
   "id": "58736753a95081d5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Batch:\n",
      "  Features Shape: torch.Size([2000, 4])\n",
      "  Labels Shape: torch.Size([2000])\n",
      "  Label Counts: tensor([ 436,  391, 1173], device='cuda:0')\n",
      "Validation Batch:\n",
      "  Features Shape: torch.Size([100000, 4])\n",
      "  Labels Shape: torch.Size([100000])\n",
      "  Label Counts: tensor([19728, 19982, 60290], device='cuda:0')\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-17T17:28:35.194144Z",
     "start_time": "2025-01-17T17:28:34.836947Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n"
   ],
   "id": "fc7844b43502ebd8",
   "outputs": [],
   "execution_count": 20
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2025-01-17T17:28:38.162212Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Training and validation loop\n",
    "num_epochs = 10000\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    print(fr\"-------------------- Epoch\", epoch+1, \"--------------------\")\n",
    "    # Training phase\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        optimizer.zero_grad()           # Zero gradients\n",
    "        outputs = model(X_batch)        # Forward pass\n",
    "        loss = criterion(outputs, y_batch)  # Compute loss\n",
    "        loss.backward()                 # Backward pass\n",
    "        optimizer.step()                # Update weights\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    train_loss = running_loss / len(train_loader)\n",
    "\n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in val_loader:\n",
    "            outputs = model(X_batch)\n",
    "            loss = criterion(outputs, y_batch)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "            # Accuracy calculation\n",
    "            predicted = outputs.argmax(dim=1)\n",
    "            correct += (predicted == y_batch).sum().item()\n",
    "            total += y_batch.size(0)\n",
    "\n",
    "    val_loss /= len(val_loader)\n",
    "    val_accuracy = correct / total\n",
    "\n",
    "    # Log metrics\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}:\")\n",
    "    print(f\"  Training Loss: {train_loss:.4f}\")\n",
    "    print(f\"  Validation Loss: {val_loss:.4f}\")\n",
    "    print(f\"  Validation Accuracy: {val_accuracy:.4f}\")\n",
    "\n",
    "    # Optional: Log GPU memory usage\n",
    "    print(f\"  GPU Memory Allocated: {torch.cuda.memory_allocated() / 1e9:.2f} GB\")\n",
    "    print(f\"  GPU Memory Reserved: {torch.cuda.memory_reserved() / 1e9:.2f} GB\")\n"
   ],
   "id": "3ebbc16e3309b380",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------- Epoch 1 --------------------\n",
      "Epoch 1/10000:\n",
      "  Training Loss: 0.4909\n",
      "  Validation Loss: 0.3956\n",
      "  Validation Accuracy: 0.8425\n",
      "  GPU Memory Allocated: 0.06 GB\n",
      "  GPU Memory Reserved: 0.16 GB\n",
      "-------------------- Epoch 2 --------------------\n",
      "Epoch 2/10000:\n",
      "  Training Loss: 0.3833\n",
      "  Validation Loss: 0.3734\n",
      "  Validation Accuracy: 0.8492\n",
      "  GPU Memory Allocated: 0.06 GB\n",
      "  GPU Memory Reserved: 0.16 GB\n",
      "-------------------- Epoch 3 --------------------\n",
      "Epoch 3/10000:\n",
      "  Training Loss: 0.3659\n",
      "  Validation Loss: 0.3594\n",
      "  Validation Accuracy: 0.8519\n",
      "  GPU Memory Allocated: 0.06 GB\n",
      "  GPU Memory Reserved: 0.16 GB\n",
      "-------------------- Epoch 4 --------------------\n",
      "Epoch 4/10000:\n",
      "  Training Loss: 0.3545\n",
      "  Validation Loss: 0.3459\n",
      "  Validation Accuracy: 0.8580\n",
      "  GPU Memory Allocated: 0.06 GB\n",
      "  GPU Memory Reserved: 0.16 GB\n",
      "-------------------- Epoch 5 --------------------\n",
      "Epoch 5/10000:\n",
      "  Training Loss: 0.3460\n",
      "  Validation Loss: 0.3399\n",
      "  Validation Accuracy: 0.8606\n",
      "  GPU Memory Allocated: 0.06 GB\n",
      "  GPU Memory Reserved: 0.16 GB\n",
      "-------------------- Epoch 6 --------------------\n",
      "Epoch 6/10000:\n",
      "  Training Loss: 0.3398\n",
      "  Validation Loss: 0.3346\n",
      "  Validation Accuracy: 0.8624\n",
      "  GPU Memory Allocated: 0.06 GB\n",
      "  GPU Memory Reserved: 0.16 GB\n",
      "-------------------- Epoch 7 --------------------\n",
      "Epoch 7/10000:\n",
      "  Training Loss: 0.3328\n",
      "  Validation Loss: 0.3279\n",
      "  Validation Accuracy: 0.8642\n",
      "  GPU Memory Allocated: 0.06 GB\n",
      "  GPU Memory Reserved: 0.16 GB\n",
      "-------------------- Epoch 8 --------------------\n",
      "Epoch 8/10000:\n",
      "  Training Loss: 0.3357\n",
      "  Validation Loss: 0.3266\n",
      "  Validation Accuracy: 0.8643\n",
      "  GPU Memory Allocated: 0.06 GB\n",
      "  GPU Memory Reserved: 0.16 GB\n",
      "-------------------- Epoch 9 --------------------\n",
      "Epoch 9/10000:\n",
      "  Training Loss: 0.3248\n",
      "  Validation Loss: 0.3225\n",
      "  Validation Accuracy: 0.8653\n",
      "  GPU Memory Allocated: 0.06 GB\n",
      "  GPU Memory Reserved: 0.16 GB\n",
      "-------------------- Epoch 10 --------------------\n",
      "Epoch 10/10000:\n",
      "  Training Loss: 0.3219\n",
      "  Validation Loss: 0.3174\n",
      "  Validation Accuracy: 0.8675\n",
      "  GPU Memory Allocated: 0.06 GB\n",
      "  GPU Memory Reserved: 0.16 GB\n",
      "-------------------- Epoch 11 --------------------\n",
      "Epoch 11/10000:\n",
      "  Training Loss: 0.3181\n",
      "  Validation Loss: 0.3159\n",
      "  Validation Accuracy: 0.8673\n",
      "  GPU Memory Allocated: 0.06 GB\n",
      "  GPU Memory Reserved: 0.16 GB\n",
      "-------------------- Epoch 12 --------------------\n",
      "Epoch 12/10000:\n",
      "  Training Loss: 0.3153\n",
      "  Validation Loss: 0.3163\n",
      "  Validation Accuracy: 0.8679\n",
      "  GPU Memory Allocated: 0.06 GB\n",
      "  GPU Memory Reserved: 0.16 GB\n",
      "-------------------- Epoch 13 --------------------\n"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "850d47e32f0b38f8"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
